{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQWFO0PsoGOI"
      },
      "source": [
        "\n",
        "\n",
        "## Getting the data (Corpus)\n",
        "\n",
        "Let us start by where we'll get our data (our **corpus**). There are many sources, but two are the most commonly used:\n",
        "* **Penn Treebank** subset from nltk (you can buy the entire Treebank, if you want, but you'll have to invest some $700~).\n",
        "* The **Universal Dependencies** Treebanks, available (as of February 2020) for 90 languages (in different quality and quantity levels).\n",
        "\n",
        "These contain the hard work of many **annotators**, which went through selected sets of sentences and annotated each one by hand, forming a corpus to be used as **supervised** input for our **machine learning algorithms**.\n",
        "\n",
        "The following two cells will show how to import the corpus from each of these two sources."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4B5gdrh7Z_Ic",
        "outputId": "7b4d3ddc-5980-46e2-cd19-8cdd0ba35431"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package treebank to C:\\Users\\Nithin\n",
            "[nltk_data]     KM\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package treebank is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "#This cell loads the Penn Treebank corpus from nltk into a list variable named penn_treebank.\n",
        "\n",
        "#No need to install nltk in google colab since it is preloaded in the environments.\n",
        "\n",
        "import nltk\n",
        "\n",
        "#Ensure that the treebank corpus is downloaded\n",
        "nltk.download('treebank')\n",
        "\n",
        "#Load the treebank corpus class\n",
        "from nltk.corpus import treebank\n",
        "\n",
        "#Now we iterate over all samples from the corpus (the fileids - that are equivalent to sentences)\n",
        "#and retrieve the word and the pre-labeled PoS tag. This will be added as a list of tuples with\n",
        "#a list of words and a list of their respective PoS tags (in the same order).\n",
        "penn_treebank = []\n",
        "for fileid in treebank.fileids():\n",
        "  tokens = []\n",
        "  tags = []\n",
        "  for word, tag in treebank.tagged_words(fileid):\n",
        "    tokens.append(word)\n",
        "    tags.append(tag)\n",
        "  penn_treebank.append((tokens, tags))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4WwZYkNr1bPN",
        "outputId": "7ad6daff-4b3e-44ce-cc23-9fcf23768478"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: conllu in c:\\users\\nithin km\\anaconda3\\lib\\site-packages (4.5.3)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'wget' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n",
            "tar: Error opening archive: Failed to open 'ud-treebanks-v2.5.tgz'\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'ud-treebanks-v2.5/UD_English-GUM/en_gum-ud-train.conllu'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\Nithin KM\\Downloads\\CRFPOS_Tagging.ipynb Cell 3\u001b[0m line \u001b[0;36m<cell line: 15>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nithin%20KM/Downloads/CRFPOS_Tagging.ipynb#W2sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mconllu\u001b[39;00m \u001b[39mimport\u001b[39;00m parse_incr\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nithin%20KM/Downloads/CRFPOS_Tagging.ipynb#W2sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m#Open the file and load the sentences to a list.\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Nithin%20KM/Downloads/CRFPOS_Tagging.ipynb#W2sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m data_file \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39mud-treebanks-v2.5/UD_English-GUM/en_gum-ud-train.conllu\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m\"\u001b[39;49m, encoding\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nithin%20KM/Downloads/CRFPOS_Tagging.ipynb#W2sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m ud_files \u001b[39m=\u001b[39m []\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nithin%20KM/Downloads/CRFPOS_Tagging.ipynb#W2sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mfor\u001b[39;00m tokenlist \u001b[39min\u001b[39;00m parse_incr(data_file):\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'ud-treebanks-v2.5/UD_English-GUM/en_gum-ud-train.conllu'"
          ]
        }
      ],
      "source": [
        "#This cell loads the Universal Dependecies Treekbank corpus. It'll download all the packages, but we'll only use the GUM\n",
        "#english package. We'll also install the conllu package, that was developed to parse data in the conLLu format, a\n",
        "#format common of linguistic annotated files. We'll also have a list variable, but now named ud_treebank.\n",
        "\n",
        "#Install conllu package, download the UD Treebanks corpus and unpack it.\n",
        "!pip install conllu\n",
        "!wget https://lindat.mff.cuni.cz/repository/xmlui/bitstream/handle/11234/1-3105/ud-treebanks-v2.5.tgz\n",
        "!tar zxf ud-treebanks-v2.5.tgz\n",
        "\n",
        "#The imports needed to open and parse (interpret) the conllu file. At the end we'll have a list of dicts.\n",
        "from io import open\n",
        "from conllu import parse_incr\n",
        "\n",
        "#Open the file and load the sentences to a list.\n",
        "data_file = open(\"ud-treebanks-v2.5/UD_English-GUM/en_gum-ud-train.conllu\", \"r\", encoding=\"utf-8\")\n",
        "ud_files = []\n",
        "for tokenlist in parse_incr(data_file):\n",
        "    ud_files.append(tokenlist)\n",
        "\n",
        "#Now we iterate over all samples from the corpus and retrieve the word and the pre-labeled PoS tag (upostag). This will\n",
        "#be added as a list of tuples with a list of words and a list of their respective PoS tags (in the same order).\n",
        "ud_treebank = []\n",
        "for sentence in ud_files:\n",
        "  tokens = []\n",
        "  tags = []\n",
        "  for token in sentence:\n",
        "    tokens.append(token['form'])\n",
        "    tags.append(token['upostag'])\n",
        "  ud_treebank.append((tokens, tags))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzzGnG10Ulv2"
      },
      "source": [
        "**Word of Caution!**\n",
        "\n",
        "Penn Treebank and UD Treebanks use *distinct tagsets*.\n",
        "\n",
        "We won't be able to interchange them unless we make a converter - also, we'll only be able to do so from Penn->UD, because Penn Treebank has tags more detailed than UD, and we won't be able to retrieve these details from the tags without a third function and a lot of effort.\n",
        "\n",
        "We'll only do that later, in our code.\n",
        "\n",
        "Let us continue with the explanation of the Tagger."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfD5ujGijuUF"
      },
      "source": [
        "#Extracting Features form Words\n",
        "\n",
        "Next, we have to create a function that is able to extract features from our words. These features will be used to predict the PoS.\n",
        "\n",
        "For that,  for each word, we'll pass the sentence and word index, and we'll provide a dict with the features.\n",
        "\n",
        "To explain about the feature set (can be changed, if you want), it is composed by:\n",
        "* Word: the word itself. Some words are always one PoS, others not.\n",
        "* is_first, is_last: check if it is the first or last in the sentence.\n",
        "* is_capitalized: first letter is caps? Maybe it is a proper noun...\n",
        "* is_all_caps or is_all_lower: checks for acronyms (or common words).\n",
        "* prefixes/suffixes: check word initialization/termination\n",
        "* prev_word/next_word: checks the preceding and succeding word.\n",
        "* has-hyphen: words with '-' may be adjectives.\n",
        "* is_numeric: for numbers.\n",
        "* capitals_inside: weird cases. Maybe nouns.\n",
        "\n",
        "The basis of this feature extraction method comes from two nice articles:\n",
        "* https://nlpforhackers.io/training-pos-tagger/\n",
        "* https://medium.com/analytics-vidhya/pos-tagging-using-conditional-random-fields-92077e5eaa31\n",
        "\n",
        "If you're wondering, yes, this encoding WILL need a lot of memory for training (if you're not using categorical variables).\n",
        "\n",
        "And we'll have to replicate this in our main code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-IyIaTSwoo-V"
      },
      "outputs": [],
      "source": [
        "#Regex module for checking alphanumeric values.\n",
        "import re\n",
        "def extract_features(sentence, index):\n",
        "  return {\n",
        "      'word':sentence[index],\n",
        "      'is_first':index==0,\n",
        "      'is_last':index ==len(sentence)-1,\n",
        "      'is_capitalized':sentence[index][0].upper() == sentence[index][0],\n",
        "      'is_all_caps': sentence[index].upper() == sentence[index],\n",
        "      'is_all_lower': sentence[index].lower() == sentence[index],\n",
        "      'is_alphanumeric': int(bool((re.match('^(?=.*[0-9]$)(?=.*[a-zA-Z])',sentence[index])))),\n",
        "      'prefix-1':sentence[index][0],\n",
        "      'prefix-2':sentence[index][:2],\n",
        "      'prefix-3':sentence[index][:3],\n",
        "      'prefix-3':sentence[index][:4],\n",
        "      'suffix-1':sentence[index][-1],\n",
        "      'suffix-2':sentence[index][-2:],\n",
        "      'suffix-3':sentence[index][-3:],\n",
        "      'suffix-3':sentence[index][-4:],\n",
        "      'prev_word':'' if index == 0 else sentence[index-1],\n",
        "      'next_word':'' if index < len(sentence) else sentence[index+1],\n",
        "      'has_hyphen': '-' in sentence[index],\n",
        "      'is_numeric': sentence[index].isdigit(),\n",
        "      'capitals_inside': sentence[index][1:].lower() != sentence[index][1:]\n",
        "  }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYS5r1_m6Yr9"
      },
      "source": [
        "We now prepare the dataset for use in Machine Learning algorithms.\n",
        "\n",
        "There are two steps (three, if we're doing deep learning, but that's for later) to it:\n",
        "* Defining a function to transform the corpus to a more datsetish format.\n",
        "* Then, divide the encoded data into training and testing sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1hiniE_wzPOC"
      },
      "outputs": [],
      "source": [
        "#Ater defining the extract_features, we define a simple function to transform our data in a more 'datasetish' format.\n",
        "#This function returns the data as two lists, one of Dicts of features and the other with the labels.\n",
        "def transform_to_dataset(tagged_sentences):\n",
        "  X, y = [], []\n",
        "  for sentence, tags in tagged_sentences:\n",
        "    sent_word_features, sent_tags = [],[]\n",
        "    for index in range(len(sentence)):\n",
        "        sent_word_features.append(extract_features(sentence, index)),\n",
        "        sent_tags.append(tags[index])\n",
        "    X.append(sent_word_features)\n",
        "    y.append(sent_tags)\n",
        "  return X, y\n",
        "\n",
        "#We divide the set BEFORE encoding. Why? To have full sentences in training/testing sets. When we encode, we do not encode\n",
        "#a sentence, but its words instead.\n",
        "\n",
        "#First, for the Penn treebank.\n",
        "penn_train_size = int(0.8*len(penn_treebank))\n",
        "penn_training = penn_treebank[:penn_train_size]\n",
        "penn_testing = penn_treebank[penn_train_size:]\n",
        "X_penn_train, y_penn_train = transform_to_dataset(penn_training)\n",
        "X_penn_test, y_penn_test = transform_to_dataset(penn_testing)\n",
        "\n",
        "#Then, for UD Treebank.\n",
        "ud_train_size = int(0.8*len(ud_treebank))\n",
        "ud_training = ud_treebank[:ud_train_size]\n",
        "ud_testing = ud_treebank[ud_train_size:]\n",
        "X_ud_train, y_ud_train = transform_to_dataset(ud_training)\n",
        "X_ud_test, y_ud_test = transform_to_dataset(ud_testing)\n",
        "\n",
        "#Third step, vectorize datasets. For that we use sklearn DictVectorizer\n",
        "#WARNING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFc51jNtDptW"
      },
      "source": [
        "# Training a Tagger\n",
        "\n",
        "Now, we can train supervised machine learning algorithms to PoS Tagging.\n",
        "\n",
        "We'll use the Conditional Random Fields (CRF) algorithm. Here's a brief explanation:\n",
        "\n",
        "* **CRF**: A variation of Markov Random Field. Okay, that might not have helped. It is a discriminative model that, in a quick summary, evaluates the probabilities that a set of states are dependant or not between themselves based on a set of observations. In this case, it evaluates the probabilities that a word observed in a context (defined by the above mentioned features) belongs to a specific PoS. In training time, it takes what is the best state given the set of current observations and probabilities.\n",
        "\n",
        "<div>\n",
        "<img src=\"https://miro.medium.com/max/681/1*8hOWH7YF5INMF2OPhKjVxA.png\" width=\"400\"/>\n",
        "</div>\n",
        "\n",
        "Want more math? Read this: https://towardsdatascience.com/conditional-random-fields-explained-e5b8256da776\n",
        "\n",
        "So, to achieve this, we'll use scikit learn (sklearn) and a sklearn compatible crf suite (skleran_crfsuit). If you don't know what is sklearn, [read this](https://scikit-learn.org/stable/getting_started.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OHTkotyWpd28",
        "outputId": "afc28188-e575-4e21-bda7-9360295caa3d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting sklearn_crfsuite\n",
            "  Downloading sklearn_crfsuite-0.3.6-py2.py3-none-any.whl (12 kB)\n",
            "Collecting python-crfsuite>=0.8.3 (from sklearn_crfsuite)\n",
            "  Downloading python_crfsuite-0.9.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (993 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m993.5/993.5 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from sklearn_crfsuite) (1.16.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from sklearn_crfsuite) (0.9.0)\n",
            "Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.10/dist-packages (from sklearn_crfsuite) (4.66.1)\n",
            "Installing collected packages: python-crfsuite, sklearn_crfsuite\n",
            "Successfully installed python-crfsuite-0.9.9 sklearn_crfsuite-0.3.6\n",
            "Started training on Penn Treebank corpus!\n",
            "Finished training on Penn Treebank corpus!\n",
            "Started training on UD corpus!\n",
            "Finished training on UD corpus!\n"
          ]
        }
      ],
      "source": [
        "#Ignoring some warnings for the sake of readability.\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "#First, install sklearn_crfsuite, as it is not preloaded into Colab.\n",
        "!pip install sklearn_crfsuite\n",
        "from sklearn_crfsuite import CRF\n",
        "\n",
        "#This loads the model. Specifics are:\n",
        "#algorithm: methodology used to check if results are improving. Default is lbfgs (gradient descent).\n",
        "#c1 and c2:  coefficients used for regularization.\n",
        "#max_iterations: max number of iterations (DUH!)\n",
        "#all_possible_transitions: since crf creates a \"network\", of probability transition states,\n",
        "#this option allows it to map even \"connections\" not present in the data.\n",
        "penn_crf = CRF(\n",
        "    algorithm='lbfgs',\n",
        "    c1=0.01,\n",
        "    c2=0.1,\n",
        "    max_iterations=100,\n",
        "    all_possible_transitions=True\n",
        ")\n",
        "#The fit method is the default name used by Machine Learning algorithms to start training.\n",
        "print(\"Started training on Penn Treebank corpus!\")\n",
        "penn_crf.fit(X_penn_train, y_penn_train)\n",
        "print(\"Finished training on Penn Treebank corpus!\")\n",
        "\n",
        "#Same for UD\n",
        "ud_crf = CRF(\n",
        "    algorithm='lbfgs',\n",
        "    c1=0.01,\n",
        "    c2=0.1,\n",
        "    max_iterations=100,\n",
        "    all_possible_transitions=True\n",
        ")\n",
        "print(\"Started training on UD corpus!\")\n",
        "ud_crf.fit(X_ud_train, y_ud_train)\n",
        "print(\"Finished training on UD corpus!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQvig1nQBcbA"
      },
      "source": [
        "# Checking the Results\n",
        "\n",
        "For that, we'll use a score method named balanced f-score. This score takes into account *precision* and *recall*.\n",
        "\n",
        "* **precision**: Considering the universe of tagged words, how many were correctly tagged?\n",
        "* **recall**: Considering the universe of correct tags, how many words were really correctly tagged?\n",
        "\n",
        "The distinction is in the direction you look. Precision looks at all tagged words to find how many are ok; Recall looks at correct tags to find how many were able to be \"guessed\".\n",
        "\n",
        "F-score is then calculated using these two. I won't go into the maths of it.  If you want,\n",
        "* You can read the wikipedia article here: https://en.wikipedia.org/wiki/F1_score\n",
        "* Or watch a neat simple video here: https://www.youtube.com/watch?v=j-EB6RqqjGI&ab_channel=CodeEmporium\n",
        "\n",
        "Also, here's the wikipedia image to help you understand:\n",
        "<div>\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/2/26/Precisionrecall.svg/350px-Precisionrecall.svg.png\"/>\n",
        "</div>\n",
        "\n",
        "We won't go into the computations either. Let the package do its thing (after all, we're interested in NLP now, not in statistics):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uTlJwNkF_0zs",
        "outputId": "117a3fd6-844f-481d-84bb-87f0a653b106"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "## Penn ##\n",
            "F1 score on Test Data\n",
            "0.9668646324625245\n",
            "F1 score on Training Data \n",
            "0.9936643188628935\n"
          ]
        }
      ],
      "source": [
        "#We'll use the sklearn_crfsuit own metrics to compute f1 score.\n",
        "from sklearn_crfsuite import metrics\n",
        "from sklearn_crfsuite import scorers\n",
        "print(\"## Penn ##\")\n",
        "\n",
        "#First calculate a prediction from test data, then we print the metrics for f-1 using the .flat_f1_score method.\n",
        "y_penn_pred=penn_crf.predict(X_penn_test)\n",
        "print(\"F1 score on Test Data\")\n",
        "print(metrics.flat_f1_score(y_penn_test, y_penn_pred,average='weighted',labels=penn_crf.classes_))\n",
        "#For the sake of clarification, we do the same for train data.\n",
        "y_penn_pred_train=penn_crf.predict(X_penn_train)\n",
        "print(\"F1 score on Training Data \")\n",
        "print(metrics.flat_f1_score(y_penn_train, y_penn_pred_train,average='weighted',labels=penn_crf.classes_))\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
